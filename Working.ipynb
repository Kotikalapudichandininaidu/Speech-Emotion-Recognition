{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxK0uj_t5C5p",
        "outputId": "bf2fc69a-78a3-4227-fb43-56be78d4c259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting json_tricks\n",
            "  Downloading json_tricks-3.17.3-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from noisereduce) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading json_tricks-3.17.3-py2.py3-none-any.whl (27 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pydub, json_tricks, noisereduce\n",
            "Successfully installed json_tricks-3.17.3 noisereduce-3.0.3 pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy json_tricks pydub librosa noisereduce tensorflow scikit-learn matplotlib seaborn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X3S-PHPQo7Wl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pydub import AudioSegment, effects\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KFpSclWIpNib"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/special project/TESS' # ADJUST THIS PATH\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/' # ADJUST THIS PATH\n",
        "PROCESSED_DATA_FILE = os.path.join(OUTPUT_DIR, 'processed_data.npz')\n",
        "MODEL_WEIGHTS_FILE = os.path.join(OUTPUT_DIR, 'best_weights_lstm_mod.keras') # Use .keras extension\n",
        "MODEL_JSON_FILE = os.path.join(OUTPUT_DIR, 'model_lstm_mod.json')\n",
        "MODEL_H5_FILE = os.path.join(OUTPUT_DIR, 'model_lstm_mod.weights.h5') # Keep h5 for compatibility if needed\n",
        "X_TEST_FILE = os.path.join(OUTPUT_DIR, 'x_test_data_mod.npy') # Save test data separately if needed\n",
        "Y_TEST_FILE = os.path.join(OUTPUT_DIR, 'y_test_data_mod.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sr5MRoi7pk5s"
      },
      "outputs": [],
      "source": [
        "FRAME_LENGTH = 2048\n",
        "HOP_LENGTH = 512\n",
        "N_MFCC = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K_anyhfWpm2E"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "BATCH_SIZE = 32 # More standard batch size\n",
        "EPOCHS = 10 # Increase epochs, use EarlyStopping\n",
        "LEARNING_RATE = 0.001 # Explicitly set learning rate if needed (Adam default is often good)\n",
        "LSTM_UNITS = 128 # Increased units\n",
        "DROPOUT_RATE = 0.3 # Added dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y9A4nBAspp9p"
      },
      "outputs": [],
      "source": [
        "# Emotion Mapping\n",
        "TESS_EMOTION_MAP = {\n",
        "    'neutral': 0, 'happy': 1, 'sad': 2, 'angry': 3,\n",
        "    'fear': 4, 'disgust': 5, 'ps': 6 # 'ps' represents surprised\n",
        "}\n",
        "EMOTION_LABELS = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "NUM_CLASSES = len(EMOTION_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CKahktAipr80"
      },
      "outputs": [],
      "source": [
        "def get_tess_emotion(filename):\n",
        "    \"\"\"Extract emotion label from TESS filename.\"\"\"\n",
        "    filename = filename.lower()\n",
        "    for emotion, label in TESS_EMOTION_MAP.items():\n",
        "        if emotion in filename:\n",
        "            return label\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Of_VkWonpuTb"
      },
      "outputs": [],
      "source": [
        "def process_audio_file(file_path, target_length, sr=None):\n",
        "    \"\"\"\n",
        "    Load, preprocess (normalize, trim, pad, noise reduce), and extract features\n",
        "    (RMS, ZCR, MFCC) from a single audio file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "        target_length (int): The desired length to pad or truncate the audio signal to.\n",
        "        sr (int, optional): Target sample rate. If None, uses the file's native SR.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Extracted features of shape (timesteps, num_features) or None if error.\n",
        "        int: The extracted emotion label or -1 if not found/error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio using librosa first to get consistent sample rate handling\n",
        "        y, sr_native = librosa.load(file_path, sr=sr) # Load with target SR if specified\n",
        "\n",
        "        # Use pydub for normalization (requires converting back to AudioSegment)\n",
        "        # Ensure sample width is appropriate (e.g., 16-bit for PCM WAV)\n",
        "        # Convert float32 numpy array to int16\n",
        "        y_int16 = (y * 32767).astype(np.int16)\n",
        "        rawsound = AudioSegment(\n",
        "            y_int16.tobytes(),\n",
        "            frame_rate=sr_native if sr is None else sr,\n",
        "            sample_width=y_int16.dtype.itemsize, # Should be 2 for int16\n",
        "            channels=1 # Assuming mono\n",
        "        )\n",
        "\n",
        "        # Normalize\n",
        "        normalizedsound = effects.normalize(rawsound, headroom=0)\n",
        "        normal_x = np.array(normalizedsound.get_array_of_samples(), dtype='float32')\n",
        "        current_sr = normalizedsound.frame_rate # Use SR from normalized sound\n",
        "\n",
        "        # Trim silence\n",
        "        xt, _ = librosa.effects.trim(normal_x, top_db=30)\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(xt) > target_length:\n",
        "            xt = xt[:target_length]\n",
        "        else:\n",
        "            xt = np.pad(xt, (0, target_length - len(xt)), 'constant')\n",
        "\n",
        "        # Noise reduction\n",
        "        # Ensure noise reduction is applied correctly, may need tuning\n",
        "        final_x = nr.reduce_noise(xt, sr=current_sr, stationary=False) # Try non-stationary\n",
        "\n",
        "        # Extract features\n",
        "        rms = librosa.feature.rms(y=final_x, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=final_x, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n",
        "        # --- Correction is here ---\n",
        "        mfccs = librosa.feature.mfcc(y=final_x, sr=current_sr, n_mfcc=N_MFCC,\n",
        "                                      n_fft=FRAME_LENGTH, hop_length=HOP_LENGTH) # Changed frame_length to n_fft\n",
        "        # --- End Correction ---\n",
        "\n",
        "        # Combine features (timesteps, features)\n",
        "        # Note: RMS and ZCR have shape (1, T) while MFCC has (n_mfcc, T). We need T steps.\n",
        "        # Transpose MFCCs and ensure shapes align. Librosa output needs careful handling.\n",
        "        # rms/zcr are often shape (1, N_FRAMES), mfcc is (N_MFCC, N_FRAMES)\n",
        "        features = np.vstack((zcr, rms, mfccs)).T # Stack vertically and transpose\n",
        "\n",
        "        # Get emotion label\n",
        "        filename = os.path.basename(file_path)\n",
        "        emotion_label = get_tess_emotion(filename)\n",
        "\n",
        "        return features, emotion_label\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\", file=sys.stderr)\n",
        "        return None, -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9jauESdTp0rR"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(dataset_path, target_length=None):\n",
        "    \"\"\"\n",
        "    Walk through the dataset directory, process each audio file,\n",
        "    and collect features and labels. Determines max length if not provided.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to the root directory of the dataset.\n",
        "        target_length (int, optional): Fixed length for audio samples. If None,\n",
        "                                       calculates max length from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Feature data (X) of shape (num_samples, timesteps, num_features).\n",
        "        np.array: Label data (Y) of shape (num_samples,).\n",
        "        int: The target length used for padding/truncating.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(dataset_path):\n",
        "        print(f\"Error: Dataset directory not found at {dataset_path}\", file=sys.stderr)\n",
        "        return None, None, 0\n",
        "\n",
        "    # --- Determine Target Length (if not provided) ---\n",
        "    if target_length is None:\n",
        "        print(\"Calculating maximum sample length...\")\n",
        "        max_len_calc = 0\n",
        "        num_files_calc = 0\n",
        "        for subdir, _, files in os.walk(dataset_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.wav', '.mp3', '.ogg')): # Add other formats if needed\n",
        "                    file_path = os.path.join(subdir, file)\n",
        "                    try:\n",
        "                        y, sr = librosa.load(file_path, sr=None)\n",
        "                        xt, _ = librosa.effects.trim(y, top_db=30)\n",
        "                        max_len_calc = max(max_len_calc, len(xt))\n",
        "                        num_files_calc += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not load/trim {file_path} during length calculation: {e}\", file=sys.stderr)\n",
        "        if max_len_calc == 0:\n",
        "             print(\"Error: Could not determine maximum length. No valid audio files found?\", file=sys.stderr)\n",
        "             return None, None, 0\n",
        "        target_length = max_len_calc\n",
        "        print(f\"Determined target length: {target_length} samples from {num_files_calc} files.\")\n",
        "\n",
        "\n",
        "    print(f\"Processing audio files using target length: {target_length}...\")\n",
        "    tic = time.perf_counter()\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    processed_count = 0\n",
        "\n",
        "    for subdir, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "             if file.lower().endswith(('.wav', '.mp3', '.ogg')):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                features, label = process_audio_file(file_path, target_length)\n",
        "\n",
        "                if features is not None and label != -1:\n",
        "                    all_features.append(features)\n",
        "                    all_labels.append(label)\n",
        "                    processed_count += 1\n",
        "                # Simple progress indicator\n",
        "                if processed_count % 100 == 0:\n",
        "                    print(f\"Processed {processed_count} files...\")\n",
        "\n",
        "\n",
        "    toc = time.perf_counter()\n",
        "    print(f\"Finished processing {processed_count} files in {(toc - tic)/60:0.4f} minutes.\")\n",
        "\n",
        "    if not all_features:\n",
        "        print(\"Error: No features were extracted.\", file=sys.stderr)\n",
        "        return None, None, target_length\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    # Ensure all feature arrays have the same number of timesteps (should be handled by process_audio_file)\n",
        "    try:\n",
        "        X = np.asarray(all_features).astype('float32')\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: Could not stack features. Check for inconsistent shapes: {e}\", file=sys.stderr)\n",
        "        # Add more debugging here if needed, e.g., print shapes of individual feature arrays\n",
        "        # for i, f in enumerate(all_features): print(f\"Shape {i}: {f.shape}\")\n",
        "        return None, None, target_length\n",
        "\n",
        "    Y = np.asarray(all_labels).astype('int8')\n",
        "\n",
        "    print(\"Feature shape (X):\", X.shape) # Should be (num_samples, timesteps, num_features)\n",
        "    print(\"Label shape (Y):\", Y.shape)   # Should be (num_samples,)\n",
        "\n",
        "    return X, Y, target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vrtRCmPKp402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781820b3-3a50-454d-f5b6-87e31ec047f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data from /content/drive/My Drive/Colab Notebooks/processed_data.npz...\n",
            "Data loaded successfully.\n",
            "Feature shape (X): (2800, 257, 15)\n",
            "Label shape (Y): (2800,)\n",
            "Target Length: 131072\n"
          ]
        }
      ],
      "source": [
        "target_len = None # Let preprocess_data calculate it initially\n",
        "if os.path.exists(PROCESSED_DATA_FILE):\n",
        "    print(f\"Loading preprocessed data from {PROCESSED_DATA_FILE}...\")\n",
        "    try:\n",
        "        data = np.load(PROCESSED_DATA_FILE, allow_pickle=True) # allow_pickle needed if contains non-numeric like target_len\n",
        "        X = data['X']\n",
        "        Y = data['Y']\n",
        "        # Load target_len if saved, otherwise it needs to be consistent\n",
        "        target_len = data['target_length'].item() if 'target_length' in data else None\n",
        "        print(\"Data loaded successfully.\")\n",
        "        print(\"Feature shape (X):\", X.shape)\n",
        "        print(\"Label shape (Y):\", Y.shape)\n",
        "        if target_len: print(\"Target Length:\", target_len)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading processed data: {e}. Reprocessing...\", file=sys.stderr)\n",
        "        X, Y, target_len = preprocess_data(DATASET_PATH, target_length=target_len) # Use previously determined length if available\n",
        "else:\n",
        "    print(\"No preprocessed data found. Starting processing...\")\n",
        "    X, Y, target_len = preprocess_data(DATASET_PATH)\n",
        "    if X is not None and Y is not None:\n",
        "        print(f\"Saving processed data to {PROCESSED_DATA_FILE}...\")\n",
        "        os.makedirs(os.path.dirname(PROCESSED_DATA_FILE), exist_ok=True)\n",
        "        np.savez(PROCESSED_DATA_FILE, X=X, Y=Y, target_length=target_len) # Save target_len too\n",
        "        print(\"Data saved.\")\n",
        "\n",
        "if X is None or Y is None:\n",
        "    print(\"Exiting due to data processing errors.\", file=sys.stderr)\n",
        "    sys.exit(1) # Exit if data processing failed\n",
        "\n",
        "if target_len is None:\n",
        "    print(\"Error: Target length could not be determined or loaded.\", file=sys.stderr)\n",
        "    # Attempt to infer from X shape if possible\n",
        "    if X is not None:\n",
        "         target_len_inferred = X.shape[1] # Assuming X is (samples, timesteps, features)\n",
        "         print(f\"Warning: Inferring target length from X shape: {target_len_inferred}\", file=sys.stderr)\n",
        "         # This might not be the original padded length, use with caution\n",
        "         # It's better to ensure target_len is saved/passed correctly.\n",
        "    else:\n",
        "        sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rYZ8rsrXsG_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33b0839-5b76-4cc1-8dda-bfaa0a63f576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data into training, validation, and test sets...\n",
            "Train set shapes: (1960, 257, 15) (1960,)\n",
            "Validation set shapes: (420, 257, 15) (420,)\n",
            "Test set shapes: (420, 257, 15) (420,)\n",
            "Saving test data...\n",
            "Test data saved.\n"
          ]
        }
      ],
      "source": [
        "# --- Data Splitting ---\n",
        "print(\"Splitting data into training, validation, and test sets...\")\n",
        "# Using a more standard 70% train, 15% validation, 15% test split\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp) # 0.5 * 0.3 = 0.15\n",
        "\n",
        "print(\"Train set shapes:\", x_train.shape, y_train.shape)\n",
        "print(\"Validation set shapes:\", x_val.shape, y_val.shape)\n",
        "print(\"Test set shapes:\", x_test.shape, y_test.shape)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_class = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_val_class = tf.keras.utils.to_categorical(y_val, NUM_CLASSES)\n",
        "y_test_class = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# --- Save Test Data (Optional but good practice) ---\n",
        "print(\"Saving test data...\")\n",
        "os.makedirs(os.path.dirname(X_TEST_FILE), exist_ok=True)\n",
        "np.save(X_TEST_FILE, x_test)\n",
        "np.save(Y_TEST_FILE, y_test) # Save original labels, not one-hot encoded\n",
        "print(\"Test data saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "piV8UcbMp7I8",
        "outputId": "d984788a-b1a2-48ee-9977-7ee75d0b3437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the LSTM model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m257\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m147,456\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m257\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m558,599\u001b[0m (2.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">558,599</span> (2.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m558,599\u001b[0m (2.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">558,599</span> (2.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(\"Building the LSTM model...\")\n",
        "model = keras.Sequential([\n",
        "    # Input shape: (timesteps, features) - determined by X.shape[1:]\n",
        "    layers.Input(shape=x_train.shape[1:]), # Explicit Input layer\n",
        "    # Consider BatchNormalization before LSTM or between LSTM layers\n",
        "    # layers.BatchNormalization(),\n",
        "    layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True)),\n",
        "    layers.Dropout(DROPOUT_RATE),\n",
        "    # layers.BatchNormalization(),\n",
        "    layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=False)), # Only last output needed here\n",
        "    layers.Dropout(DROPOUT_RATE),\n",
        "    # layers.BatchNormalization(),\n",
        "    # Add an intermediate Dense layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(DROPOUT_RATE),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), # Using Adam\n",
        "              metrics=['categorical_accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wYSDoJKr1qP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2eb434e-8d56-4af6-954e-571352c985ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - categorical_accuracy: 0.2490 - loss: 1.8748 - val_categorical_accuracy: 0.4881 - val_loss: 1.3560 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - categorical_accuracy: 0.4271 - loss: 1.4748 - val_categorical_accuracy: 0.4881 - val_loss: 1.2867 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - categorical_accuracy: 0.4966 - loss: 1.2763 - val_categorical_accuracy: 0.6190 - val_loss: 1.0487 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - categorical_accuracy: 0.6220 - loss: 1.0392 - val_categorical_accuracy: 0.6905 - val_loss: 0.8972 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - categorical_accuracy: 0.6802 - loss: 0.8512 - val_categorical_accuracy: 0.7881 - val_loss: 0.5608 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m43/62\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 2s/step - categorical_accuracy: 0.7655 - loss: 0.6534"
          ]
        }
      ],
      "source": [
        "# --- Callbacks ---\n",
        "# Save only the best model based on validation accuracy\n",
        "mcp_save = callbacks.ModelCheckpoint(\n",
        "    MODEL_WEIGHTS_FILE, save_best_only=True,\n",
        "    monitor='val_categorical_accuracy', mode='max', save_weights_only=False # Save entire model\n",
        ")\n",
        "# Reduce learning rate if validation accuracy plateaus\n",
        "rlrop = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_categorical_accuracy', factor=0.2, patience=10, min_lr=0.00001, verbose=1\n",
        ")\n",
        "# Stop training early if validation loss doesn't improve\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=15, verbose=1, restore_best_weights=True # Restore best weights found\n",
        ")\n",
        "\n",
        "# --- Training ---\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(x_train, y_train_class,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=(x_val, y_val_class),\n",
        "                    callbacks=[mcp_save, rlrop, early_stopping]) # Added early stopping\n",
        "\n",
        "# Note: If EarlyStopping restored best weights, loading from checkpoint might be redundant,\n",
        "# but it's safer if saving the whole model fails for some reason.\n",
        "# Check if the best model was saved and load it explicitly if needed.\n",
        "if os.path.exists(MODEL_WEIGHTS_FILE) and early_stopping.stopped_epoch > 0: # Check if early stopping happened\n",
        "     print(f\"Loading best model weights from epoch {early_stopping.best_epoch + 1} saved at {MODEL_WEIGHTS_FILE}\")\n",
        "     # If mcp_save saved the whole model:\n",
        "     model = keras.models.load_model(MODEL_WEIGHTS_FILE)\n",
        "     # If save_weights_only=True was used with ModelCheckpoint:\n",
        "     # model.load_weights(MODEL_WEIGHTS_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwK3grycr3_8"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    \"\"\"Plots training & validation loss and accuracy.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    ax1.plot(history.history['loss'], label='Loss (training data)')\n",
        "    ax1.plot(history.history['val_loss'], label='Loss (validation data)')\n",
        "    ax1.set_title('Loss for Train and Validation Sets')\n",
        "    ax1.set_ylabel('Loss Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "\n",
        "    # Plot Accuracy\n",
        "    ax2.plot(history.history['categorical_accuracy'], label='Accuracy (training data)')\n",
        "    ax2.plot(history.history['val_categorical_accuracy'], label='Accuracy (validation data)')\n",
        "    ax2.set_title('Accuracy for Train and Validation Sets')\n",
        "    ax2.set_ylabel('Accuracy (%)') # Accuracy is usually 0-1, unless multiplied by 100\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Plotting training history...\")\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6do0CcdLr72X"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, x_data, y_data_class, set_name=\"Validation\"):\n",
        "    \"\"\"Evaluates the model and prints/plots confusion matrix.\"\"\"\n",
        "    print(f\"\\n--- {set_name} Set Evaluation ---\")\n",
        "    loss, acc = model.evaluate(x_data, y_data_class, verbose=0)\n",
        "    print(f\"{set_name} Loss: {loss:.4f}\")\n",
        "    print(f\"{set_name} Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(x_data)\n",
        "    y_pred_class = np.argmax(predictions, axis=1)\n",
        "    y_true_class = np.argmax(y_data_class, axis=1) # Convert one-hot back to class indices\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = sklearn.metrics.confusion_matrix(y_true_class, y_pred_class)\n",
        "    cm_df = pd.DataFrame(cm, index=EMOTION_LABELS, columns=EMOTION_LABELS)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f'{set_name} Set Confusion Matrix')\n",
        "    plt.ylabel('True Emotion')\n",
        "    plt.xlabel('Predicted Emotion')\n",
        "    plt.show()\n",
        "\n",
        "    # Per-class accuracy\n",
        "    values = cm.diagonal()\n",
        "    row_sum = np.sum(cm, axis=1)\n",
        "    # Handle division by zero for classes with no samples in the set\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        class_acc = values / row_sum\n",
        "        class_acc[np.isnan(class_acc)] = 0.0 # Set NaN to 0\n",
        "\n",
        "    print(f'\\n{set_name} Set Predicted Emotions Accuracy per Class:')\n",
        "    for i, emotion in enumerate(EMOTION_LABELS):\n",
        "        if row_sum[i] > 0: # Only print if class exists in this set\n",
        "             print(f\"{emotion}: {class_acc[i]:.4f} ({values[i]}/{row_sum[i]})\")\n",
        "        else:\n",
        "             print(f\"{emotion}: N/A (0 samples)\")\n",
        "\n",
        "# Evaluate on Validation Set\n",
        "evaluate_model(model, x_val, y_val_class, set_name=\"Validation\")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "evaluate_model(model, x_test, y_test_class, set_name=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jViT7Y3r-Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa0c8cd-7404-4b29-9700-166a19115bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving the final model...\n",
            "Saved model architecture to /content/drive/My Drive/Colab Notebooks/model_lstm_mod.json\n",
            "Saved model weights to /content/drive/My Drive/Colab Notebooks/model_lstm_mod.weights.h5\n",
            "Best model also saved/loaded from /content/drive/My Drive/Colab Notebooks/best_weights_lstm_mod.keras (Keras format)\n"
          ]
        }
      ],
      "source": [
        "# --- Save Final Model ---\n",
        "# Save in Keras native format (recommended) and potentially JSON/H5 for compatibility\n",
        "print(\"\\nSaving the final model...\")\n",
        "# Keras format (saves architecture, weights, optimizer state)\n",
        "# model.save(MODEL_WEIGHTS_FILE.replace('.keras', '_final.keras')) # Save final explicitly if needed\n",
        "\n",
        "# JSON (architecture) + H5 (weights)\n",
        "model_json = model.to_json()\n",
        "os.makedirs(os.path.dirname(MODEL_JSON_FILE), exist_ok=True)\n",
        "with open(MODEL_JSON_FILE, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(MODEL_H5_FILE)\n",
        "print(f\"Saved model architecture to {MODEL_JSON_FILE}\")\n",
        "print(f\"Saved model weights to {MODEL_H5_FILE}\")\n",
        "print(f\"Best model also saved/loaded from {MODEL_WEIGHTS_FILE} (Keras format)\")\n",
        "\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_emotion_from_file(audio_file_path, model_to_use, target_len_predict):\n",
        "    \"\"\"Predicts emotion from a single audio file using the trained model.\"\"\"\n",
        "    print(f\"\\nPredicting emotion for: {audio_file_path}\")\n",
        "    if not os.path.exists(audio_file_path):\n",
        "        print(\"Error: Audio file not found.\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "    # Process the audio file using the same parameters as training\n",
        "    features, _ = process_audio_file(audio_file_path, target_length=target_len_predict)\n",
        "\n",
        "    if features is None:\n",
        "        print(\"Error: Could not extract features from the audio file.\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "    # Add batch dimension: (timesteps, features) -> (1, timesteps, features)\n",
        "    features = np.expand_dims(features, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model_to_use.predict(features)\n",
        "    predicted_index = np.argmax(predictions, axis=1)[0]\n",
        "    predicted_emotion = EMOTION_LABELS[predicted_index]\n",
        "    confidence_scores = predictions[0]\n",
        "\n",
        "    print(f\"Predicted Emotion: {predicted_emotion}\")\n",
        "    print(\"Confidence Scores:\")\n",
        "    for emotion, score in zip(EMOTION_LABELS, confidence_scores):\n",
        "        print(f\"  {emotion}: {score:.4f}\")\n",
        "\n",
        "    return predicted_emotion, confidence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eOIhmiCr_Pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9406b05c-fef7-47d2-c9ac-60aaddba719d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicting emotion for: /content/drive/MyDrive/special project/archive/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "Predicted Emotion: neutral\n",
            "Confidence Scores:\n",
            "  neutral: 0.8087\n",
            "  happy: 0.0002\n",
            "  sad: 0.1831\n",
            "  angry: 0.0004\n",
            "  fearful: 0.0021\n",
            "  disgust: 0.0006\n",
            "  surprised: 0.0048\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "# --- Example Usage for Prediction ---\n",
        "# Make sure the example file path is correct\n",
        "EXAMPLE_AUDIO_FILE = '/content/drive/MyDrive/special project/archive/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav' # ADJUST THIS PATH\n",
        "\n",
        "# Ensure target_len is available for prediction\n",
        "if target_len is None:\n",
        "    print(\"Warning: Target length for prediction is unknown. Results may be inaccurate.\", file=sys.stderr)\n",
        "    # You might need to load it from the saved model or data file if running prediction separately\n",
        "else:\n",
        "    predict_emotion_from_file(EXAMPLE_AUDIO_FILE, model, target_len)\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}